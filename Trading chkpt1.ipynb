{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPEHifXXIY2Li4r0ryaCmQN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vorlde/-WQU-MScFE-Capstone-Project/blob/main/Trading%20chkpt1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install dill\n",
        "! pip install statsmodels\n",
        "!pip install hurst\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.tsa.stattools import adfuller as ADF_test\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NowgSYVYyAcs",
        "outputId": "c607daf3-ac2d-49a3-ad62-1c574f71c211"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting dill\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: dill\n",
            "Successfully installed dill-0.3.8\n",
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.10/dist-packages (0.14.2)\n",
            "Requirement already satisfied: numpy>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from statsmodels) (1.25.2)\n",
            "Requirement already satisfied: scipy!=1.9.2,>=1.8 in /usr/local/lib/python3.10/dist-packages (from statsmodels) (1.11.4)\n",
            "Requirement already satisfied: pandas!=2.1.0,>=1.4 in /usr/local/lib/python3.10/dist-packages (from statsmodels) (2.0.3)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.10/dist-packages (from statsmodels) (0.5.6)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from statsmodels) (24.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2024.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy>=0.5.6->statsmodels) (1.16.0)\n",
            "Collecting hurst\n",
            "  Downloading hurst-0.0.5-py3-none-any.whl (5.9 kB)\n",
            "Requirement already satisfied: pandas>=0.18 in /usr/local/lib/python3.10/dist-packages (from hurst) (2.0.3)\n",
            "Requirement already satisfied: numpy>=1.10 in /usr/local/lib/python3.10/dist-packages (from hurst) (1.25.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.18->hurst) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.18->hurst) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.18->hurst) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=0.18->hurst) (1.16.0)\n",
            "Installing collected packages: hurst\n",
            "Successfully installed hurst-0.0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zl31PRcFBJPH",
        "outputId": "3508adfc-8d0f-4614-f5bc-40e0904630cb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "tZKyEVxTA7ZL"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Define the file path for saving the model\n",
        "file_path = \"/content/drive/My Drive/Model/hedge_ratio_model.h5\"\n",
        "\n",
        "# To load the model later\n",
        "loaded_model = load_model(file_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "\n",
        "from sklearn.cluster import KMeans, DBSCAN\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn import preprocessing\n",
        "\n",
        "from statsmodels.tsa.stattools import coint\n",
        "\n",
        "from scipy import stats\n",
        "\n",
        "import lzma\n",
        "import dill as pickle\n",
        "\n",
        "def save_pickle(path,obj):\n",
        "    with lzma.open(path,\"wb\") as fp:\n",
        "        pickle.dump(obj,fp)\n",
        "\n",
        "def load_pickle(path):\n",
        "    with lzma.open(path,\"rb\") as fp:\n",
        "        file = pickle.load(fp)\n",
        "    return file\n",
        "\n",
        "def clean_data(ticker_dfs,tickers):\n",
        "    intraday_range = ticker_dfs[tickers[0]].index\n",
        "    for inst in tickers:\n",
        "        ticker_dfs[inst] = ticker_dfs[inst].reindex(intraday_range)\n",
        "    closes = []\n",
        "\n",
        "    for tk in tickers:\n",
        "        close = ticker_dfs[tk].close\n",
        "        closes.append(close)\n",
        "\n",
        "    pricing = pd.concat(closes,axis = 1)\n",
        "    pricing.columns = tickers\n",
        "\n",
        "    return pricing\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def get_pca_features(ret_df,N_PRIN_COMPONENTS =10):\n",
        "\n",
        "    pca = PCA(n_components=N_PRIN_COMPONENTS)\n",
        "    pca.fit(ret_df)\n",
        "\n",
        "    # Extract factor loadings\n",
        "    factor_loadings = pca.components_.T  # Transpose the components matrix\n",
        "\n",
        "    # Create a DataFrame with the correct orientation\n",
        "    factor_loadings_df = pd.DataFrame(factor_loadings, index=ret_df.columns, columns=[f'Factor {i+1}' for i in range(N_PRIN_COMPONENTS)])\n",
        "\n",
        "    X = preprocessing.StandardScaler().fit_transform(pca.components_.T)\n",
        "\n",
        "    return X\n",
        "\n",
        "def create_clusters(X,index):\n",
        "    clf = DBSCAN(eps=1, min_samples=3)\n",
        "\n",
        "    print(clf)\n",
        "\n",
        "    clf.fit(X)\n",
        "    labels = clf.labels_\n",
        "    n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
        "    print(\"\\nClusters discovered: %d\" % n_clusters_)\n",
        "\n",
        "    clustered = clf.labels_\n",
        "\n",
        "    clustered_series = pd.Series(index=index, data=clustered.flatten())\n",
        "    clustered_series = clustered_series[clustered_series != -1]\n",
        "\n",
        "    return clustered_series\n",
        "\n",
        "\n",
        "def find_cointegrated_pairs(data, significance=0.05):\n",
        "    # This function is from https://www.quantopian.com/lectures/introduction-to-pairs-trading\n",
        "    n = data.shape[1]\n",
        "    score_matrix = np.zeros((n, n))\n",
        "    pvalue_matrix = np.ones((n, n))\n",
        "    keys = data.keys()\n",
        "    pairs = []\n",
        "    for i in range(n):\n",
        "        for j in range(i+1, n):\n",
        "            S1 = data[keys[i]]\n",
        "            S2 = data[keys[j]]\n",
        "            result = coint(S1, S2)\n",
        "            score = result[0]\n",
        "            pvalue = result[1]\n",
        "            score_matrix[i, j] = score\n",
        "            pvalue_matrix[i, j] = pvalue\n",
        "            if pvalue < significance:\n",
        "                pairs.append((keys[i], keys[j]))\n",
        "    return score_matrix, pvalue_matrix, pairs\n",
        "\n",
        "\n",
        "\n",
        "def get_coint_pairs(prices,clustered_series):\n",
        "\n",
        "    valid_tickers = clustered_series.index.intersection(prices.columns)\n",
        "    clustered_series = clustered_series.loc[valid_tickers]\n",
        "\n",
        "    CLUSTER_SIZE_LIMIT = 9999\n",
        "    counts = clustered_series.value_counts()\n",
        "    ticker_count_reduced = counts[(counts>1) & (counts<=CLUSTER_SIZE_LIMIT)]\n",
        "\n",
        "    cluster_dict = {}\n",
        "    for i, which_clust in enumerate(ticker_count_reduced.index):\n",
        "        tickers = clustered_series[clustered_series == which_clust].index\n",
        "        score_matrix, pvalue_matrix, pairs = find_cointegrated_pairs(\n",
        "            prices[tickers]\n",
        "        )\n",
        "        cluster_dict[which_clust] = {}\n",
        "        cluster_dict[which_clust]['score_matrix'] = score_matrix\n",
        "        cluster_dict[which_clust]['pvalue_matrix'] = pvalue_matrix\n",
        "        cluster_dict[which_clust]['pairs'] = pairs\n",
        "\n",
        "    pairs = []\n",
        "    for clust in cluster_dict.keys():\n",
        "        pairs.extend(cluster_dict[clust]['pairs'])\n",
        "\n",
        "    return pairs\n",
        "\n",
        "\n",
        "def get_data():\n",
        "    data_path = \"/content/drive/My Drive/constituents.csv\"\n",
        "    dfs_path = \"/content/drive/My Drive/new_dfs.obj\"\n",
        "\n",
        "    ticker_dfs = load_pickle(dfs_path)\n",
        "    snp_data = pd.read_csv(data_path)\n",
        "    tickers = []\n",
        "\n",
        "    for i in range(499):\n",
        "      tickers.append(snp_data.Symbol[i])\n",
        "\n",
        "    tickers.remove(\"BF.B\")\n",
        "    tickers.remove(\"BRK.B\")\n",
        "    tickers.remove(\"CPAY\")\n",
        "    tickers.remove(\"DAY\")\n",
        "    tickers.remove(\"GEV\")\n",
        "    tickers.remove(\"SOLV\")\n",
        "\n",
        "    return tickers,ticker_dfs"
      ],
      "metadata": {
        "id": "Kbr_2DXIB1qh"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tickers,ticker_dfs = get_data()"
      ],
      "metadata": {
        "id": "fNOBSlToxu7b"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_trading_data_blocks(df, month,year):\n",
        "    if month == 12:\n",
        "      month = 1\n",
        "      year +=1\n",
        "\n",
        "    else:\n",
        "      month+=1\n",
        "\n",
        "    month_data =df[(df.index.year == year) & (df.index.month == month)]\n",
        "    coint_check_data = month_data[:500]\n",
        "    trading = month_data\n",
        "\n",
        "    return coint_check_data,trading\n",
        "\n",
        "\n",
        "def calculate_monthly_clusters(monthly_data):\n",
        "    \"\"\"\n",
        "    Calculate clusters for the given month.\n",
        "\n",
        "    \"\"\"\n",
        "    ret_df = monthly_data.pct_change().round(4).fillna(0)\n",
        "    X = get_pca_features(ret_df)\n",
        "    clustered_series = create_clusters(X, ret_df.columns)\n",
        "\n",
        "    return clustered_series\n",
        "\n",
        "\n",
        "def yield_monthly_data(df):\n",
        "    \"\"\"\n",
        "    Yields each month's data from the dataframe.\n",
        "    \"\"\"\n",
        "    for year in df.index.year.unique():\n",
        "        for month in df[df.index.year == year].index.month.unique():\n",
        "            monthly_data = df[(df.index.year == year) & (df.index.month == month)]\n",
        "            yield year, month, monthly_data"
      ],
      "metadata": {
        "id": "gy25yBD1-GgC"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import statsmodels.api as sm\n",
        "from hurst import compute_Hc  # Assuming you have a function to compute the Hurst exponent\n",
        "from statsmodels.tools.tools import add_constant\n",
        "from scipy.stats import skew, kurtosis\n",
        "\n",
        "def add_stationary(spread_normalized):\n",
        "\n",
        "    rolling_volatility = spread_normalized[:500].rolling(window=10).std()\n",
        "    hurst_exponent = compute_Hc(spread_normalized[:500])[0]  # Hurst exponent\n",
        "\n",
        "    skewness = skew(spread_normalized[:500])\n",
        "    kurtosis_value = kurtosis(spread_normalized[:500])\n",
        "\n",
        "\n",
        "    # Ensure spread_lagged starts from the second element to align with spread_diff which starts from the second element due to differencing and shifting\n",
        "    spread_lagged = spread_normalized.shift(1).dropna()\n",
        "    spread_diff = spread_normalized.diff().dropna()  # First difference to get spread_diff\n",
        "\n",
        "    # Make sure indices are aligned by reindexing spread_diff to match spread_lagged\n",
        "    spread_diff = spread_diff.reindex(spread_lagged.index)\n",
        "    spread_diff += 1e-8\n",
        "\n",
        "    # Now perform your regression with aligned indices\n",
        "    model = sm.OLS(np.log(np.abs(spread_diff)), add_constant(spread_lagged))\n",
        "    result = model.fit()\n",
        "    phi = result.params[0]\n",
        "\n",
        "\n",
        "    spread_normalized = spread_normalized[:500]  # Normalized spread\n",
        "\n",
        "    features_df = pd.DataFrame({\n",
        "        'spread_normalized': spread_normalized,\n",
        "        'rolling_volatility': rolling_volatility[:500],\n",
        "        'hurst_exponent': np.repeat(hurst_exponent, rolling_volatility.shape[0]),\n",
        "    })\n",
        "\n",
        "    # Mean Reversion Speed calculated as the absolute value of phi\n",
        "    mean_reversion_speed = np.abs(phi)\n",
        "\n",
        "    # Calculate half-life from the decay factor\n",
        "    half_life = -np.log(2) / np.log(np.abs(phi))\n",
        "\n",
        "\n",
        "    features_df['half_life'] = np.repeat(half_life, len(features_df))\n",
        "    features_df['mean_reversion_speed'] = np.repeat(mean_reversion_speed, len(features_df))\n",
        "    features_df['skewness'] = np.repeat(skewness, len(features_df))\n",
        "    features_df['kurtosis'] = np.repeat(kurtosis_value, len(features_df))\n",
        "\n",
        "    return features_df\n",
        "\n",
        "\n",
        "\n",
        "# def process_pair(pair, prices):\n",
        "#     s1, s2 = pair\n",
        "\n",
        "#     datax = prices[s1]\n",
        "#     datay = prices[s2]\n",
        "\n",
        "#     # Find the hedge ratio using OLS on the first 500 data points\n",
        "#     hedge_ratio = sm.OLS(datax[:500], sm.add_constant(datay[:500])).fit().params[1]\n",
        "\n",
        "#     # Calculate the normalized spread based on the hedge ratio\n",
        "#     spread = datax - hedge_ratio * datay\n",
        "#     spread_normalized = (spread - spread.mean()) / spread.std()\n",
        "\n",
        "#     features_df = add_stationary(spread_normalized)\n",
        "#     features_df['hedge_ratio'] = np.repeat(hedge_ratio, len(features_df))\n",
        "\n",
        "#     # Perform ADF test on the first 500 points for stationarity\n",
        "#     adf_result_train = ADF_test(spread_normalized)\n",
        "#     print(pair, adf_result_train[1], )\n",
        "\n",
        "#     features_df.dropna(inplace = True)\n",
        "\n",
        "\n",
        "#     # Proceed only if the first 500 points spread is stationary\n",
        "#     if adf_result_train[1] < 0.05:\n",
        "#         stationarity_train = adf_result_train[1] < 0.05\n",
        "\n",
        "#         return features_df\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def process_pair(pair, prices):\n",
        "    s1, s2 = pair\n",
        "\n",
        "    datax = prices[s1]\n",
        "    datay = prices[s2]\n",
        "\n",
        "    # Calculate hedge ratio using the first 500 data points\n",
        "    hedge_ratio = sm.OLS(datax[:500], sm.add_constant(datay[:500])).fit().params[1]\n",
        "\n",
        "    # Calculate the spread based on the hedge ratio\n",
        "    spread = datax - hedge_ratio * datay\n",
        "\n",
        "    # Calculate mean and standard deviation using the first 500 data points\n",
        "    initial_mean = spread[:500].mean()\n",
        "    initial_std = spread[:500].std()\n",
        "\n",
        "    # Create a DataFrame with the calculated columns\n",
        "    df = pd.DataFrame({\n",
        "        'datax': datax,\n",
        "        'datay': datay,\n",
        "        'hedge_ratio': hedge_ratio,\n",
        "        'spread': spread,\n",
        "        'rolling_mean': initial_mean,\n",
        "        'rolling_std': initial_std\n",
        "    })\n",
        "\n",
        "    # Normalized spread\n",
        "    spread_normalized = (spread - initial_mean) / initial_std\n",
        "\n",
        "    # Plotting the normalized spread along with mean and ±2 std lines\n",
        "    plt.figure(figsize=(15, 7))\n",
        "    spread_normalized = spread_normalized.reset_index(drop=True)\n",
        "    plt.title(f'Z-Scores for {s1} and {s2} Across Training Period')\n",
        "    spread_normalized.plot(label='Normalized Spread')\n",
        "    plt.axhline(y=0, color='black', linestyle='--', label='Mean')\n",
        "    plt.axhline(y=2, color='red', linestyle='--', label='+2 Std Dev')\n",
        "    plt.axhline(y=-2, color='green', linestyle='--', label='-2 Std Dev')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "dnqIEf0a9yqd"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main_driver_function(prices):\n",
        "\n",
        "    for year, month, monthly_data in yield_monthly_data(prices):\n",
        "\n",
        "        print(f\"Clustering for {month} {year}\")\n",
        "        cluster = calculate_monthly_clusters(monthly_data)\n",
        "\n",
        "        for block_data in generate_trading_data_blocks(prices, month, year):\n",
        "            if block_data.shape[0] == 624:\n",
        "                block_data = block_data.dropna(axis=1)\n",
        "                pairs = get_coint_pairs(block_data, cluster)\n",
        "\n",
        "                features_list, stationarity_labels, hedge_ratios = [], [], []\n",
        "                for pair in pairs:\n",
        "                    result = process_pair(pair, block_data)\n",
        "                    if result is not None:\n",
        "                        features_df, stationarity_test, hedge_ratio_test = result\n",
        "                        features_list.append(features_df)\n",
        "                        stationarity_labels.append(stationarity_test)\n",
        "                        hedge_ratios.append(hedge_ratio_test)\n",
        "\n"
      ],
      "metadata": {
        "id": "BNUEtPYLyopr"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "intraday_dfs = ticker_dfs.copy()\n",
        "prices = clean_data(intraday_dfs, tickers)\n",
        "\n",
        "# main_driver_function(prices[-100:])"
      ],
      "metadata": {
        "id": "bvror4abyd0i"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def calculate_spread(data, pair):\n",
        "    y = data[pair[0]]\n",
        "    x = data[pair[1]]\n",
        "    return y - x\n",
        "\n",
        "def calculate_rolling_stats(spread, lookback):\n",
        "    rolling_mean = spread.rolling(window=lookback).mean()\n",
        "    rolling_std = spread.rolling(window=lookback).std()\n",
        "    return rolling_mean, rolling_std\n",
        "\n",
        "def generate_signals(spread, rolling_mean, rolling_std, enter_threshold, exit_threshold):\n",
        "    signals = pd.DataFrame(index=spread.index)\n",
        "    signals['spread'] = spread\n",
        "    signals['upper_limit'] = rolling_mean + enter_threshold * rolling_std\n",
        "    signals['lower_limit'] = rolling_mean - enter_threshold * rolling_std\n",
        "    signals['up_medium'] = rolling_mean + exit_threshold * rolling_std\n",
        "    signals['low_medium'] = rolling_mean - exit_threshold * rolling_std\n",
        "\n",
        "    signals['position'] = 0\n",
        "    signals.loc[spread > signals['upper_limit'], 'position'] = -1\n",
        "    signals.loc[spread < signals['lower_limit'], 'position'] = 1\n",
        "    signals.loc[(spread < signals['up_medium']) & (spread > signals['low_medium']), 'position'] = 0\n",
        "\n",
        "    signals['position'] = signals['position'].ffill().shift().fillna(0)\n",
        "\n",
        "    return signals\n",
        "\n",
        "def trade_pairs(pair, trading_data, lookback=500, enter_threshold=2, exit_threshold=0.5):\n",
        "    spread = calculate_spread(trading_data, pair)\n",
        "    rolling_mean, rolling_std = calculate_rolling_stats(spread, lookback)\n",
        "\n",
        "    signals = generate_signals(spread, rolling_mean, rolling_std, enter_threshold, exit_threshold)\n",
        "\n",
        "    trading_data['returns'] = trading_data[pair[0]].pct_change() - trading_data[pair[1]].pct_change()\n",
        "    trading_data['strategy_returns'] = signals['position'] * trading_data['returns']\n",
        "\n",
        "    trading_data['cumulative_returns'] = (1 + trading_data['strategy_returns']).cumprod() - 1\n",
        "    total_profit = trading_data['cumulative_returns'].iloc[-1]\n",
        "\n",
        "    return total_profit, trading_data\n"
      ],
      "metadata": {
        "id": "z2dlSuaS88DG"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "data = prices[-5000:]\n",
        "\n",
        "for year, month, monthly_data in yield_monthly_data(data):\n",
        "    print(f\"Clustering for {month} {year}\")\n",
        "    cluster = calculate_monthly_clusters(monthly_data)\n",
        "    data = data.dropna(axis=1)\n",
        "    coint_check_data, trading_data = generate_trading_data_blocks(data, month,year)\n",
        "    pairs = get_coint_pairs(coint_check_data, cluster)\n",
        "\n",
        "    print(pairs)\n",
        "\n",
        "    #trade these pairs\n",
        "    for pair in pairs:\n",
        "      profits, results_df = trade_pairs(pair, trading_data)\n",
        "      print(f'Total profit for the pair {pair}: {profits}')\n",
        "      # print(results_df.tail())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 827
        },
        "id": "bWigLgH2_dQw",
        "outputId": "97b2ce8d-9d4f-44c5-9591-1bae38140763"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clustering for 9 2023\n",
            "DBSCAN(eps=1, min_samples=3)\n",
            "\n",
            "Clusters discovered: 5\n",
            "[]\n",
            "Clustering for 10 2023\n",
            "DBSCAN(eps=1, min_samples=3)\n",
            "\n",
            "Clusters discovered: 11\n",
            "[]\n",
            "Clustering for 11 2023\n",
            "DBSCAN(eps=1, min_samples=3)\n",
            "\n",
            "Clusters discovered: 7\n",
            "[('MMM', 'PFE'), ('GOOGL', 'GOOG'), ('GOOG', 'PFE'), ('BAC', 'KO'), ('BAC', 'PFE'), ('BAC', 'VZ')]\n",
            "Total profit for the pair ('MMM', 'PFE'): -0.02821281423895361\n",
            "Total profit for the pair ('GOOGL', 'GOOG'): 0.004340305410650114\n",
            "Total profit for the pair ('GOOG', 'PFE'): -0.006918976380576036\n",
            "Total profit for the pair ('BAC', 'KO'): -0.013552159194277436\n",
            "Total profit for the pair ('BAC', 'PFE'): -0.0775502061888721\n",
            "Total profit for the pair ('BAC', 'VZ'): -0.05930318032167492\n",
            "Clustering for 12 2023\n",
            "DBSCAN(eps=1, min_samples=3)\n",
            "\n",
            "Clusters discovered: 11\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "zero-size array to reduction operation maximum which has no identity",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-32a940c3798d>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mcoint_check_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrading_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_trading_data_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myear\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mpairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_coint_pairs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoint_check_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcluster\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpairs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-bbd81f41044d>\u001b[0m in \u001b[0;36mget_coint_pairs\u001b[0;34m(prices, clustered_series)\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhich_clust\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mticker_count_reduced\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0mtickers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclustered_series\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclustered_series\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mwhich_clust\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m         score_matrix, pvalue_matrix, pairs = find_cointegrated_pairs(\n\u001b[0m\u001b[1;32m    113\u001b[0m             \u001b[0mprices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtickers\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         )\n",
            "\u001b[0;32m<ipython-input-4-bbd81f41044d>\u001b[0m in \u001b[0;36mfind_cointegrated_pairs\u001b[0;34m(data, significance)\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0mS1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0mS2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mS1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0mpvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/statsmodels/tsa/stattools.py\u001b[0m in \u001b[0;36mcoint\u001b[0;34m(y0, y1, trend, method, maxlag, autolag, return_results)\u001b[0m\n\u001b[1;32m   1799\u001b[0m         \u001b[0mxx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1800\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1801\u001b[0;31m         \u001b[0mxx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madd_trend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprepend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1802\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1803\u001b[0m     \u001b[0mres_co\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOLS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/statsmodels/tsa/tsatools.py\u001b[0m in \u001b[0;36madd_trend\u001b[0;34m(x, trend, prepend, has_constant)\u001b[0m\n\u001b[1;32m    132\u001b[0m             \u001b[0mcol_const\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msafe_is_const\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m             \u001b[0mptp0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mptp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m             \u001b[0mcol_is_const\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mptp0\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m             \u001b[0mnz_const\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcol_is_const\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mptp\u001b[0;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[1;32m   2682\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2683\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mptp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2684\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_methods\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ptp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2685\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_ptp\u001b[0;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_ptp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m     return um.subtract(\n\u001b[0;32m--> 220\u001b[0;31m         \u001b[0mumr_maximum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m         \u001b[0mumr_minimum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: zero-size array to reduction operation maximum which has no identity"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install backtrader"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rWx9BB7b3ke6",
        "outputId": "e70874f0-42ab-4499-db15-33307050a6e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting backtrader\n",
            "  Downloading backtrader-1.9.78.123-py2.py3-none-any.whl (419 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m419.5/419.5 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: backtrader\n",
            "Successfully installed backtrader-1.9.78.123\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mvUIVplb_zl6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O5Ee_cJzLTH3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}